# news_rss_api_upd5.py - –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π MDI

import feedparser
import requests
import json
import time
from datetime import datetime, timedelta, timezone
import re
from bs4 import BeautifulSoup
import asyncio
import aiohttp
from dotenv import load_dotenv
import os
import numpy as np
# import pandas as pd # –£–±—Ä–∞–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é
from textblob import TextBlob
import warnings
import urllib3
warnings.filterwarnings('ignore')
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # –û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL

# --- –ñ–ï–°–¢–ö–û –ó–ê–î–ê–ù–ù–´–ô –ö–õ–Æ–ß API (–¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Ä–∞–±–æ—Ç—ã) ---
# –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –µ–≥–æ —Å–ª–µ–¥—É–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –≤ —Å–µ–∫—Ä–µ—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ .env
HARDCODED_NEWSAPI_KEY = 'f7e7ee9e68c64bebab1f6447c9ecd67b'
# ----------------------------------------------------

# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

class MDIAnalyzer:
    def __init__(self):
        self.mdi_components = {
            'PM200': {
                'components': {
                    '4,4-mdi': 0.775, '2,2-mdi': 0.02, '2,4-mdi': 0.035,
                    'oligomers': 0.125, 'ureidone': 0.015, 'carbodiimide': 0.0075,
                    'alkyl_substituted': 0.003, 'other_isocyanates': 0.002,
                    'impurities': 0.004, 'organic_impurities': 0.0035
                },
                'region': 'China',
                'producer': 'Wanhua',
                'type': 'Polymeric MDI'
            },
            '44V20L': {
                'components': {
                    '4,4-mdi': 0.97, '2,2-mdi': 0.0075, '2,4-mdi': 0.015,
                    'oligomers': 0.015, 'ureidone': 0.0075, 'carbodiimide': 0.003,
                    'alkyl_substituted': 0.002, 'other_isocyanates': 0.0015,
                    'impurities': 0.0005, 'organic_impurities': 0.002
                },
                # –£—Ç–æ—á–Ω–µ–Ω–∏–µ: 44V20L –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –≤ –ö–∏—Ç–∞–µ –∏ –∑–∞–∫—É–ø–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ö–∏—Ç–∞–µ
                'region': 'China',
                'producer': 'Covestro',
                'type': 'Pure MDI'
            }
        }
        
        self.raw_materials = {
            'aniline': 0.8, 'phosgene': 0.7, 'benzene': 0.6, 'gas': 0.5, 'chlorine': 0.4
        }

    def analyze_sentiment(self, text):
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            return polarity
        except Exception as e:
            return 0.0

class NewsIntegrator:
    def __init__(self):
        # --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ RSS-–∫–∞–Ω–∞–ª–æ–≤ ---
        self.rss_feeds = {
            'chemical_industry_global': [
                'https://www.icis.com/news/rss/chemicals/', # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
                # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Ñ–∏–¥—ã, –µ—Å–ª–∏ –Ω–∞–π–¥—É—Ç—Å—è
                'https://www.chemicalweek.com/feeds/home', # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
            ],
            'energy_raw_materials': [
                'https://oilprice.com/rss/main',
            ],
            'specialized_chemical': [
                # 'https://www.petrochemicalupdate.com/feed/', # Timeout —Ä–∞–Ω–µ–µ
            ]
            # 'russian_news': [...] —É–±—Ä–∞–Ω, —Ñ–æ–∫—É—Å –Ω–∞ –ö–∏—Ç–∞–µ
        }
        
        # --- –õ–û–ì–ò–ö–ê –ü–û–õ–£–ß–ï–ù–ò–Ø –ö–õ–Æ–ß–ê API ---
        # 1. –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–π –∫–ª—é—á
        if not self.newsapi_key:
            self.newsapi_key = HARDCODED_NEWSAPI_KEY
            print("‚ö†Ô∏è  –ö–ª—é—á NewsAPI –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–π –∫–ª—é—á.")
        else:
            print("‚úÖ –ö–ª—é—á NewsAPI —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
        # ----------------------------------
        
        # --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ---
        self.keywords = {
            'high_impact': [
                '—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä', 'force majeure', 'plant shutdown', 'production halt', 'emergency stop', 'Á¥ßÊÄ•ÂÅúËΩ¶', 'ÂÅú‰∫ß',
                'maintenance', 'turnaround', 'Ê£Ä‰øÆ', 'Â§ß‰øÆ', 'outage', 'startup', '–∑–∞–ø—É—Å–∫', 'ÂêØÂä®'
            ],
            'medium_impact': [
                '—Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ', '—Ä–æ—Å—Ç —Ü–µ–Ω', '–¥–µ—Ñ–∏—Ü–∏—Ç', '–ø–æ—Å—Ç–∞–≤–∫–∏', '—ç–∫—Å–ø–æ—Ä—Ç',
                'raw material prices', 'supply shortage', 'export restrictions',
                'ÂéüÊñô‰ª∑Ê†º', '‰æõÂ∫îÁ¥ßÂº†', 'Âá∫Âè£ÈôêÂà∂', '‰∫ßËÉΩ', '‰∫ßÈáè'
            ],
            'low_impact': [
                '–ø—Ä–æ–≥–Ω–æ–∑', '–∞–Ω–∞–ª–∏–∑', '—Ä—ã–Ω–æ–∫', '—Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏', 'market analysis',
                'forecast', 'trends', 'market conditions',
                'Â∏ÇÂú∫ÂàÜÊûê', 'Ë∂ãÂäø', 'È¢ÑÊµã'
            ],
            'logistics': [
                '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–ø–æ—Ä—Ç', '–∂/–¥',
                'shipping', 'logistics', 'transportation', 'railway', 'port',
                'Áâ©ÊµÅ', 'ËøêËæì', 'Ê∏ØÂè£', 'ÈìÅË∑Ø'
            ],
            'regional': {
                'China': ['–ö–∏—Ç–∞–π', 'Wanhua', 'PM200', '44V20L', '–®–∞–Ω—Ö–∞–π', '–¶–∏–Ω–¥–∞–æ', '‰∏áÂçéÂåñÂ≠¶', 'ÁÉüÂè∞', 'ÂÆÅÊ≥¢', 'Á¶èÂª∫', 'ÁßëÊÄùÂàõ', 'Covestro China', '‰∫®ÊñØËøà', 'Huntsman'],
                'Asia': ['–ê–∑–∏—è', '‰∫öÊ¥≤', '‰∫öÂ§™'],
            },
            # --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ---
            'main_keywords': [
                # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –º–∞—Ä–∫–∏ - —Ñ–æ–∫—É—Å –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ
                'mdi', 'mdipm200', '44v20l', 'wanhua', '‰∏áÂçéÂåñÂ≠¶', 'covestro', 'ÁßëÊÄùÂàõ', 'huntsman', '‰∫®ÊñØËøà', 'basf', 'Â∑¥ÊñØÂ§´',
                'methylenediphenyl diisocyanate', '–ø–æ–ª–∏–º–µ—Ä–Ω—ã–π mdi', '—á–∏—Å—Ç—ã–π mdi', 'supranal', 'isonate',
                'ËÅöÊ∞®ÈÖØ', '‰∫åËãØÂü∫Áî≤ÁÉ∑‰∫åÂºÇÊ∞∞ÈÖ∏ÈÖØ', 'mdi polymer', 'mdi pure',
                # –°—ã—Ä—å–µ
                'aniline', 'phosgene', 'benzene', 'gas', '—Ö–ª–æ—Ä', '–∞–Ω–∏–ª–∏–Ω', '—Ñ–æ—Å–≥–µ–Ω', '–±–µ–Ω–∑–æ–ª', '–≥–∞–∑',
                'Ê∞ØÊ∞î', 'ËãØ', 'ÂÖâÊ∞î', 'Â§©ÁÑ∂Ê∞î', 'natural gas', 'crude benzene', 'Á°ùÂü∫ËãØ', 'nitrobenzene',
                # –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∏ –ø–æ—Å—Ç–∞–≤–∫–∏ - —Ñ–æ–∫—É—Å –Ω–∞ –ö–∏—Ç–∞–π
                'production', 'supply', 'capacity', 'outage', 'output', 'plant', 'facility', 'unit',
                '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–ø–æ—Å—Ç–∞–≤–∫–∞', '–º–æ—â–Ω–æ—Å—Ç—å', '–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ', '–≤—ã–ø—É—Å–∫', '–∑–∞–≤–æ–¥', '—É—Å—Ç–∞–Ω–æ–≤–∫–∞',
                'Áîü‰∫ß', '‰æõÂ∫î', '‰∫ßËÉΩ', 'ÂÅúËΩ¶', '‰∫ßÈáè', 'Ë£ÖÁΩÆ', 'Â∑•ÂéÇ', 'ÂçïÂÖÉ',
                # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
                'force majeure', 'maintenance', 'shutdown', 'startup', 'expansion', 'debottlenecking', 'turnaround', 'revamp',
                '—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä', '—Ä–µ–º–æ–Ω—Ç', '–æ—Å—Ç–∞–Ω–æ–≤–∫–∞', '–∑–∞–ø—É—Å–∫', '—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ', '–º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è',
                'Á¥ßÊÄ•ÂÅúËΩ¶', 'Ê£Ä‰øÆ', 'Êâ©‰∫ß', 'Êñ∞‰∫ßËÉΩ', 'ÊäÄÊîπ', 'Â§ß‰øÆ', 'ÂºÄËΩ¶', 'Êäï‰∫ß',
                # –¢–æ—Ä–≥–æ–≤–ª—è –∏ —Ü–µ–Ω—ã
                'price', 'pricing', 'contract price', 'spot price', 'export', 'import', 'trade',
                '—Ü–µ–Ω–∞', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω–∞—è —Ü–µ–Ω–∞', '—Å–ø–æ—Ç–æ–≤–∞—è —Ü–µ–Ω–∞', '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç', '—Ç–æ—Ä–≥–æ–≤–ª—è',
                '‰ª∑Ê†º', 'ÂêàÂêå‰ª∑', 'Áé∞Ë¥ß‰ª∑', 'Âá∫Âè£', 'ËøõÂè£', 'Ë¥∏Êòì',
                # –õ–æ–≥–∏—Å—Ç–∏–∫–∞ –∏ —Ä–µ–≥–∏–æ–Ω—ã –ö–∏—Ç–∞—è
                'logistics', 'shipment', 'delivery', 'port', 'transportation',
                '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–æ—Ç–≥—Ä—É–∑–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–ø–æ—Ä—Ç', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞',
                'Áâ©ÊµÅ', 'Ë£ÖËøê', '‰∫§‰ªò', 'Ê∏ØÂè£', 'ËøêËæì',
                '–®–∞–Ω—Ö–∞–π', '–¶–∏–Ω–¥–∞–æ', '–ù–∞–Ω–∫–∏–Ω', '–¢—è–Ω—å—Ü–∑–∏–Ω—å', 'ÂÆÅÊ≥¢', 'ÁÉüÂè∞', '‰∏äÊµ∑', 'ÈùíÂ≤õ', 'Âçó‰∫¨', 'Â§©Ê¥•'
            ]
        }
        
        # –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.mdi_keywords = list(set(
            self.get_all_keywords()
        ))
        print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.mdi_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")

    def get_all_keywords(self):
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        keywords = []
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords.extend(self.keywords['main_keywords'])
        # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for region_keywords in self.keywords['regional'].items():
            # region_keywords —ç—Ç–æ tuple (–∫–ª—é—á, –∑–Ω–∞—á–µ–Ω–∏–µ)
            # –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–µ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤)
            if isinstance(region_keywords, tuple):
                keywords.extend(region_keywords[1])
            else:
                 # –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ (–æ—à–∏–±–∫–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∫–æ–¥–µ)
                 keywords.extend(region_keywords)
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å—ã—Ä—å—è
        keywords.extend(['aniline', 'phosgene', 'benzene', 'gas', '—Ö–ª–æ—Ä', '–∞–Ω–∏–ª–∏–Ω', '—Ñ–æ—Å–≥–µ–Ω', '–±–µ–Ω–∑–æ–ª', '–≥–∞–∑', 'Ê∞ØÊ∞î', 'ËãØ', 'ÂÖâÊ∞î', 'Â§©ÁÑ∂Ê∞î', 'natural gas', 'crude benzene', 'Á°ùÂü∫ËãØ'])
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
        for impact_keywords in [self.keywords['high_impact'], self.keywords['medium_impact'], self.keywords['low_impact'], self.keywords['logistics']]:
            keywords.extend(impact_keywords)
        return keywords

    async def fetch_rss_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Ñ–∏–¥–æ–≤"""
        all_news = []
        total_feeds = sum(len(feeds) for feeds in self.rss_feeds.values())
        if total_feeds > 0:
            print(f"üì° –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {total_feeds} RSS —Ñ–∏–¥–æ–≤...")
        else:
            print("üì° –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö RSS —Ñ–∏–¥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞.")
            return all_news
        
        for category, feeds in self.rss_feeds.items():
            if not feeds:
                continue
            print(f"   üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category} ({len(feeds)} —Ñ–∏–¥–æ–≤)")
            for feed_url in feeds:
                try:
                    print(f"      üîç –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º: {feed_url}")
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL
                    response = requests.get(feed_url, timeout=15, verify=False)
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        entries_count = len(feed.entries)
                        print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {entries_count} –∑–∞–ø–∏—Å–µ–π")
                        for entry in feed.entries[:20]: # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–æ 20
                            pub_date = None
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                try:
                                    pub_date = datetime(*entry.published_parsed[:6])
                                    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ aware datetime (UTC)
                                    if pub_date.tzinfo is None:
                                        pub_date = pub_date.replace(tzinfo=timezone.utc)
                                except Exception as e:
                                    pub_date = datetime.now(timezone.utc) # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
                            else:
                                pub_date = datetime.now(timezone.utc) # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
                            
                            news_item = {
                                'title': getattr(entry, 'title', ''),
                                'content': getattr(entry, 'summary', ''),
                                'date': pub_date,
                                'source': getattr(feed.feed, 'title', 'RSS Feed'),
                                'url': getattr(entry, 'link', ''),
                                'category': category
                            }
                            all_news.append(news_item)
                    else:
                        print(f"      ‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code} –¥–ª—è {feed_url}")
                except Exception as e:
                    print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ RSS {feed_url}: {type(e).__name__}")
                    continue # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —Ñ–∏–¥–æ–º
        
        print(f"   üì• –í—Å–µ–≥–æ –∏–∑ RSS: {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return all_news

    async def fetch_newsapi_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NewsAPI —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        news = []
        print("üì∞ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NewsAPI —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π...")
        
        if not self.newsapi_key or self.newsapi_key == 'demo':
            print("   ‚ö†Ô∏è NewsAPI –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ")
            return news
            
        try:
            from_date = (datetime.now(timezone.utc) - timedelta(days=5)).strftime('%Y-%m-%d') # –£–≤–µ–ª–∏—á–∏–º –ø–µ—Ä–∏–æ–¥
            
            # 1. –ó–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è MDI/Wanhua/Covestro China
            print("   üîç –ó–∞–ø—Ä–æ—Å 1: –ù–æ–≤–æ—Å—Ç–∏ MDI/Wanhua/Covestro China –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–Ω–µ–π")
            search_terms_mdi = [
                'MDI', 'Wanhua', '‰∏áÂçéÂåñÂ≠¶', 'Covestro', 'ÁßëÊÄùÂàõ', '44V20L', 'PM200',
                'ËÅöÊ∞®ÈÖØ', '‰∫åËãØÂü∫Áî≤ÁÉ∑‰∫åÂºÇÊ∞∞ÈÖ∏ÈÖØ', 'Huntsman', '‰∫®ÊñØËøà'
            ]
            search_query_mdi = ' OR '.join(search_terms_mdi)
            
            url_mdi = "https://newsapi.org/v2/everything"
            params_mdi = {
                'q': search_query_mdi,
                'from': from_date,
                'sortBy': 'publishedAt',
                'pageSize': 40, # –£–≤–µ–ª–∏—á–∏–º
                'page': 1,
                'apiKey': self.newsapi_key
            }
            
            response_mdi = requests.get(url_mdi, params=params_mdi, timeout=20)
            if response_mdi.status_code == 200:
                data_mdi = response_mdi.json()
                articles_fetched_mdi = len(data_mdi.get('articles', []))
                print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {articles_fetched_mdi} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ MDI/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è–º")
                for article in data_mdi.get('articles', []):
                    try:
                        pub_date_str = article['publishedAt']
                        if pub_date_str.endswith('Z'):
                            pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
                        else:
                            pub_date = datetime.fromisoformat(pub_date_str)
                        
                        news.append({
                            'title': article['title'],
                            'content': article['description'] or '',
                            'date': pub_date,
                            'source': article['source']['name'],
                            'url': article['url'],
                            'category': 'newsapi_mdi_specific'
                        })
                    except Exception as e:
                        continue
            else:
                print(f"      ‚ùå NewsAPI –æ—à–∏–±–∫–∞ (MDI –∑–∞–ø—Ä–æ—Å): {response_mdi.status_code} - {response_mdi.text}")

            # 2. –ó–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π –æ —Å—ã—Ä—å–µ (–∞–Ω–∞–ª–∏–Ω, –±–µ–Ω–∑–æ–ª, –≥–∞–∑) - —Ñ–æ–∫—É—Å –Ω–∞ –ö–∏—Ç–∞–π
            print("   üîç –ó–∞–ø—Ä–æ—Å 2: –ù–æ–≤–æ—Å—Ç–∏ –æ —Å—ã—Ä—å–µ (–∞–Ω–∞–ª–∏–Ω, –±–µ–Ω–∑–æ–ª, –≥–∞–∑) —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –ö–∏—Ç–∞–π")
            search_terms_raw = [
                'aniline', 'benzene', 'natural gas', 'Ê∞ØÊ∞î', 'ËãØ', 'Â§©ÁÑ∂Ê∞î', 'crude benzene', 'Á°ùÂü∫ËãØ'
            ]
            search_query_raw = ' OR '.join(search_terms_raw)
            search_query_raw_with_region = f"({search_query_raw}) AND (China OR ‰∏≠ÂõΩ OR –ö–∏—Ç–∞–π)"
            
            url_raw = "https://newsapi.org/v2/everything"
            params_raw = {
                'q': search_query_raw_with_region,
                'from': from_date,
                'sortBy': 'publishedAt',
                'pageSize': 30, # –£–≤–µ–ª–∏—á–∏–º
                'page': 1,
                'apiKey': self.newsapi_key
            }
            
            response_raw = requests.get(url_raw, params=params_raw, timeout=20)
            if response_raw.status_code == 200:
                data_raw = response_raw.json()
                articles_fetched_raw = len(data_raw.get('articles', []))
                print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {articles_fetched_raw} –Ω–æ–≤–æ—Å—Ç–µ–π –æ —Å—ã—Ä—å–µ (—Å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–æ–∫—É—Å–æ–º)")
                for article in data_raw.get('articles', []):
                    try:
                        pub_date_str = article['publishedAt']
                        if pub_date_str.endswith('Z'):
                            pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
                        else:
                            pub_date = datetime.fromisoformat(pub_date_str)
                        
                        news.append({
                            'title': article['title'],
                            'content': article['description'] or '',
                            'date': pub_date,
                            'source': article['source']['name'],
                            'url': article['url'],
                            'category': 'newsapi_raw_materials'
                        })
                    except Exception as e:
                        continue
            else:
                print(f"      ‚ùå NewsAPI –æ—à–∏–±–∫–∞ (—Å—ã—Ä—å–µ –∑–∞–ø—Ä–æ—Å): {response_raw.status_code} - {response_raw.text}")

            # 3. –ó–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–ª—é—á–µ–≤—ã—Ö —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
            print("   üîç –ó–∞–ø—Ä–æ—Å 3: –ù–æ–≤–æ—Å—Ç–∏ —Å –∫–ª—é—á–µ–≤—ã—Ö —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–Ω–µ–π)")
            # --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ ---
            domains = "icis.com,platts.com,chemanalyst.com,chemicalweek.com,hydrocarbonprocessing.com,polymerupdate.com"
            url_domains = "https://newsapi.org/v2/everything"
            params_domains = {
                'domains': domains,
                'from': from_date,
                'sortBy': 'publishedAt',
                'pageSize': 30, # –£–≤–µ–ª–∏—á–∏–º
                'page': 1,
                'apiKey': self.newsapi_key
            }
            
            response_domains = requests.get(url_domains, params=params_domains, timeout=20)
            if response_domains.status_code == 200:
                data_domains = response_domains.json()
                articles_fetched_domains = len(data_domains.get('articles', []))
                print(f"      ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {articles_fetched_domains} –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
                for article in data_domains.get('articles', []):
                    try:
                        pub_date_str = article['publishedAt']
                        if pub_date_str.endswith('Z'):
                            pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
                        else:
                            pub_date = datetime.fromisoformat(pub_date_str)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –ª–∏ –¥–ª—è MDI (–ø—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
                        title_content = (article['title'] or '') + ' ' + (article['description'] or '')
                        if any(kw in title_content.lower() for kw in ['mdi', 'polyurethane', 'diisocyanate', 'wanhua', '‰∏áÂçé', 'covestro', 'ÁßëÊÄùÂàõ', 'huntsman', '‰∫®ÊñØËøà']):
                            news.append({
                                'title': article['title'],
                                'content': article['description'] or '',
                                'date': pub_date,
                                'source': article['source']['name'],
                                'url': article['url'],
                                'category': 'newsapi_chemical_sites'
                            })
                    except Exception as e:
                        continue
            else:
                print(f"      ‚ùå NewsAPI –æ—à–∏–±–∫–∞ (–¥–æ–º–µ–Ω—ã –∑–∞–ø—Ä–æ—Å): {response_domains.status_code} - {response_domains.text}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π NewsAPI: {type(e).__name__}")
        
        unique_news = list({v['url']:v for v in news}.values()) # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ URL
        print(f"   üì• –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ NewsAPI: {len(unique_news)}")
        return unique_news

    async def fetch_chinese_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ (–ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥)"""
        chinese_news = []
        print("üá®üá≥ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤...")
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ pudaily (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ)
        # pudaily.com –ø–æ—Ö–æ–∂–µ –Ω–µ –æ—Ç–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ requests (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π body)
        # –ü–æ–ø—Ä–æ–±—É–µ–º chem99, –∫–æ—Ç–æ—Ä—ã–π —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –µ—Å—Ç—å –ø—É–±–ª–∏—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        try:
            # –ü—Ä–∏–º–µ—Ä: –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É chem99
            site_url = 'https://www.chem99.com/'
            print(f"   üîç –ü–∞—Ä—Å–∏–º {site_url} (–≤—Ä—É—á–Ω—É—é)")
            async with aiohttp.ClientSession() as session:
                async with session.get(site_url, timeout=15, ssl=False) as response:
                    if response.status == 200:
                        html = await response.text()
                        # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è MDI –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
                        # –°–∞–π—Ç chem99 —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –º—ã –ø–æ–ª—É—á–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞
                        # –ù–æ –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –µ—Å—Ç—å –ø—É–±–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...
                        if 'mdi' in html.lower() or '‰∏áÂçé' in html or 'ÁßëÊÄùÂàõ' in html:
                            title = "–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ MDI –Ω–∞ Chem99 (—Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞)"
                            content = "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Chem99 —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å MDI. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è."
                            pub_date = datetime.now(timezone.utc)
                            
                            chinese_news.append({
                                'title': title,
                                'content': content,
                                'date': pub_date,
                                'source': 'Chem99 (Manual Check)',
                                'url': site_url,
                                'category': 'chinese_news_manual'
                            })
                            print(f"      ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –Ω–∞ {site_url}")
                        else:
                            print(f"      ‚ÑπÔ∏è  –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ {site_url}")
                    else:
                        print(f"      ‚ùå –û—à–∏–±–∫–∞ HTTP {response.status} –¥–ª—è {site_url}")
        except Exception as e:
            print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site_url}: {type(e).__name__}")
            
        print(f"   üì• –í—Å–µ–≥–æ —Å –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤: {len(chinese_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return chinese_news

    def is_relevant_to_mdi(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Ç–µ–º–µ MDI"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.mdi_keywords)

    async def fetch_all_news_sources(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("üì° –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        tasks = [
            self.fetch_rss_news(),
            self.fetch_newsapi_news(), # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
            self.fetch_chinese_news()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_news = []
        source_names = ['RSS', 'NewsAPI', 'Chinese Sites']
        active_sources = 0
        for i, result in enumerate(results):
            if isinstance(result, list):
                count = len(result)
                print(f"   üì• –ò–∑ {source_names[i]} –ø–æ–ª—É—á–µ–Ω–æ: {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
                all_news.extend(result)
                if count > 0:
                    active_sources += 1
            elif isinstance(result, Exception):
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –∏–∑ {source_names[i]}: {type(result).__name__}")
        
        if active_sources == 0:
             print("   ‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        print(f"üì• –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
        return all_news

    def extract_timing_info(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        timing_patterns = {
            'immediate': ['–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '—Å–µ–π—á–∞—Å', 'urgent', 'immediately', 'Á´ãÂç≥', 'È©¨‰∏ä'],
            'short_term': ['–±–ª–∏–∂–∞–π—à–∏–µ –¥–Ω–∏', 'next few days', 'within a week', '–Ω–µ–¥–µ–ª—é', 'Âá†Â§©ÂÜÖ', '‰∏ÄÂë®ÂÜÖ'],
            'medium_term': ['2-4 –Ω–µ–¥–µ–ª–∏', '2-4 weeks', 'next month', '–º–µ—Å—è—Ü', 'Âá†Âë®', '‰∏Ä‰∏™Êúà'],
            'long_term': ['–Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Å—è—Ü–µ–≤', 'several months', 'long term', '–¥–ª–∏—Ç–µ–ª—å–Ω—ã–π', 'Âá†‰∏™Êúà', 'ÈïøÊúü']
        }
        
        text_lower = text.lower()
        timing_info = {}
        
        for period, keywords in timing_patterns.items():
            found = any(keyword in text_lower for keyword in keywords)
            if found:
                timing_info[period] = True
        
        numbers = re.findall(r'(\d+)\s*(–¥–µ–Ω—å|–¥–Ω—è|–¥–Ω–µ–π|day|days|–Ω–µ–¥–µ–ª—è|–Ω–µ–¥–µ–ª–∏|weeks|week|Â§©|Âë®)', text_lower)
        if numbers:
            timing_info['specific_numbers'] = numbers
            
        return timing_info

    def predict_impact_timing(self, news_item):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è —Å–æ–±—ã—Ç–∏—è"""
        text = (news_item['title'] or '') + ' ' + (news_item.get('content', '') or '')
        timing_info = self.extract_timing_info(text)
        
        base_timing = {
            'production_halt': 0,      # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ
            'maintenance': 7,          # 1 –Ω–µ–¥–µ–ª—è
            'raw_material_price': 3,   # 3 –¥–Ω—è
            'logistics_issue': 5,      # 5 –¥–Ω–µ–π
            'market_analysis': 14     # 2 –Ω–µ–¥–µ–ª–∏
        }
        
        text_lower = text.lower()
        impact_timing = 1  # –î–Ω–∏ –¥–æ –≤–ª–∏—è–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        
        if any(word in text_lower for word in ['—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä', 'force majeure', 'halt', 'stop', 'Á¥ßÊÄ•ÂÅúËΩ¶', 'ÂÅú‰∫ß']):
            impact_timing = base_timing['production_halt']
        elif any(word in text_lower for word in ['—Ä–µ–º–æ–Ω—Ç', 'maintenance', 'Ê£Ä‰øÆ', 'Â§ß‰øÆ']):
            impact_timing = base_timing['maintenance']
        elif any(word in text_lower for word in ['—Ü–µ–Ω—ã', 'price', '–∞–Ω–∞–ª–∏–Ω', 'aniline', '–±–µ–Ω–∑–æ–ª', 'benzene', '–≥–∞–∑', 'gas', 'ÂéüÊñô', '‰ª∑Ê†º']):
            impact_timing = base_timing['raw_material_price']
        elif any(word in text_lower for word in ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', 'shipping', 'logistics', 'Áâ©ÊµÅ', 'ËøêËæì']):
            impact_timing = base_timing['logistics_issue']
        else:
            impact_timing = base_timing['market_analysis']
        
        if 'specific_numbers' in timing_info:
            try:
                number, unit = timing_info['specific_numbers'][0]
                number = int(number)
                if any(u in unit.lower() for u in ['–¥–µ–Ω—å', 'day', 'Â§©']):
                    impact_timing = number
                elif any(u in unit.lower() for u in ['–Ω–µ–¥–µ–ª', 'week', 'Âë®']):
                    impact_timing = number * 7
            except Exception as e:
                pass
        
        return impact_timing

    def filter_mdi_relevant_news(self, all_news):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö MDI —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏"""
        if not all_news:
            print("üîç –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
            return []
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏...")
        relevant_news = []
        
        for news in all_news:
            title_lower = (news['title'] or '').lower()
            content_lower = (news.get('content', '') or '').lower()
            full_text = title_lower + ' ' + content_lower
            
            relevance_score = 0.0 # –ù–∞—á–∏–Ω–∞–µ–º —Å 0.0
            matched_keywords = []
            
            # --- –£—Ç–æ—á–Ω–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –±–∞–ª–ª–æ–≤ ---
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for keyword in self.keywords['main_keywords']:
                if keyword.lower() in full_text:
                    if keyword.lower() in ['mdi', '–º–¥–∏']:
                        relevance_score += 2
                        matched_keywords.append(keyword)
                    elif keyword.lower() in ['wanhua', '‰∏áÂçéÂåñÂ≠¶', 'covestro', 'ÁßëÊÄùÂàõ']:
                        relevance_score += 5 # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
                        matched_keywords.append(keyword)
                    elif keyword.lower() in ['44v20l', 'pm200']:
                        relevance_score += 4 # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è –º–∞—Ä–æ–∫
                        matched_keywords.append(keyword)
                    elif keyword.lower() in ['–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', 'production', '—Ä–µ–º–æ–Ω—Ç', 'maintenance', '—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä', 'force majeure', '–æ—Å—Ç–∞–Ω–æ–≤–∫–∞', 'shutdown', '—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ', 'expansion', 'Ê£Ä‰øÆ', 'Â§ß‰øÆ', 'Á¥ßÊÄ•ÂÅúËΩ¶', 'Êâ©‰∫ß', 'ÂºÄËΩ¶', 'Êäï‰∫ß']:
                        relevance_score += 3 # –°—Ä–µ–¥–Ω–∏–π –≤—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è —Å–æ–±—ã—Ç–∏–π
                        matched_keywords.append(keyword)
                    elif keyword.lower() in ['–∞–Ω–∏–ª–∏–Ω', 'aniline', '—Ñ–æ—Å–≥–µ–Ω', 'phosgene', '–±–µ–Ω–∑–æ–ª', 'benzene', '–≥–∞–∑', 'gas', 'Ê∞ØÊ∞î', 'ËãØ', 'ÂÖâÊ∞î', 'Â§©ÁÑ∂Ê∞î', 'natural gas', 'crude benzene', 'Á°ùÂü∫ËãØ']:
                        relevance_score += 3 # –°—Ä–µ–¥–Ω–∏–π –≤—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è —Å—ã—Ä—å—è
                        matched_keywords.append(keyword)
                    elif keyword.lower() in ['—Ü–µ–Ω—ã', 'price', '–ø–æ—Å—Ç–∞–≤–∫–∏', 'supply', '–¥–µ—Ñ–∏—Ü–∏—Ç', 'shortage', '‰∫ßËÉΩ', '‰∫ßÈáè', 'Áé∞Ë¥ß‰ª∑', 'ÂêàÂêå‰ª∑']:
                        relevance_score += 1.5 # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å –¥–ª—è —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
                        matched_keywords.append(keyword)
                    else:
                        relevance_score += 1
                        matched_keywords.append(keyword)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ñ–æ–∫—É—Å –Ω–∞ –ö–∏—Ç–∞–µ)
            # –£—Å–∏–ª–∏–≤–∞–µ–º –æ–±—â–∏–π —Å–∫–æ—Ä, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
            regional_boost = 1.0
            for region, region_keywords in self.keywords['regional'].items():
                for keyword in region_keywords:
                    if keyword.lower() in full_text:
                        regional_boost *= 1.2 # –£—Å–∏–ª–µ–Ω–∏–µ –Ω–∞ 20% –∑–∞ –∫–∞–∂–¥–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
                        matched_keywords.append(f"{keyword}({region})")
            relevance_score *= regional_boost

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è (–±–æ–ª–µ–µ –¥–µ–ª–∏–∫–∞—Ç–Ω–æ–µ –Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ)
            for impact_category in [self.keywords['high_impact'], self.keywords['medium_impact'], self.keywords['low_impact'], self.keywords['logistics']]:
                for keyword in impact_category:
                    if keyword.lower() in full_text:
                        if keyword.lower() in self.keywords['high_impact']:
                            relevance_score += 1.5
                        elif keyword.lower() in self.keywords['medium_impact']:
                            relevance_score += 1.0
                        elif keyword.lower() in self.keywords['low_impact']:
                            relevance_score += 0.5
                        elif keyword.lower() in self.keywords['logistics']:
                            relevance_score += 0.7
                        matched_keywords.append(keyword)
            
            # --- –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ ---
            if relevance_score >= 2.5: # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                # ... (–æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫)
                impact_timing = self.predict_impact_timing(news)
                news_date = news['date']
                if news_date.tzinfo is None:
                    news_date = news_date.replace(tzinfo=timezone.utc)
                impact_date = news_date + timedelta(days=impact_timing)
                
                news['relevance_score'] = relevance_score
                news['matched_keywords'] = matched_keywords
                news['impact_timing_days'] = impact_timing
                news['predicted_impact_date'] = impact_date
                news['impact_timeline'] = self.get_timeline_category(impact_timing)
                
                relevant_news.append(news)
        
        relevant_news.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        print(f"üéØ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(relevant_news)}")
        return relevant_news

    def get_timeline_category(self, days):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫"""
        if days <= 1:
            return 'immediate'
        elif days <= 7:
            return 'short_term'
        elif days <= 30:
            return 'medium_term'
        else:
            return 'long_term'

    def get_news_with_timeline(self, hours=120): # 5 –¥–Ω–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø—Ä–æ–≥–Ω–æ–∑–æ–º –≤—Ä–µ–º–µ–Ω–∏ –≤–ª–∏—è–Ω–∏—è"""
        print(f"üïí –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours} —á–∞—Å–æ–≤...")
        async def fetch_and_filter():
            all_news = await self.fetch_all_news_sources()
            relevant_news = self.filter_mdi_relevant_news(all_news)
            
            time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours)
            recent_news = [
                news for news in relevant_news 
                if isinstance(news['date'], datetime) and news['date'] > time_threshold
            ]
            
            print(f"üìÖ –û—Ç–æ–±—Ä–∞–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ—Å–ª–µ {time_threshold.strftime('%Y-%m-%d %H:%M')} UTC): {len(recent_news)}")
            return recent_news
        
        return asyncio.run(fetch_and_filter())

class EnhancedMDIAnalyzer(MDIAnalyzer):
    def __init__(self):
        super().__init__()
        self.news_integrator = NewsIntegrator()
    
    def fetch_news(self, days_back=5): # 5 –¥–Ω–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            recent_news = self.news_integrator.get_news_with_timeline(hours=days_back*24)
            if len(recent_news) == 0:
                print("   ‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return recent_news
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {type(e).__name__}")
            return []

    def analyze_news_item(self, news_item):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        title = news_item.get('title', '')
        content = news_item.get('content', '')
        full_text = (title or '') + ' ' + (content or '')
        
        sentiment = self.analyze_sentiment(full_text)
        impact_score = min(news_item.get('relevance_score', 0) / 10, 1.0)
        total_impact = impact_score * (1 + sentiment * 0.2)
        total_impact = max(0, min(total_impact, 1))
        
        return {
            'news_item': news_item,
            'sentiment': sentiment,
            'impact_score': impact_score,
            'total_impact': total_impact,
            'analysis_date': datetime.now(timezone.utc)
        }

    def run_full_analysis(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ (–ë–ï–ó –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–Ω)"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –ò–ò-–∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞ MDI...")
        print("=" * 60)
        print("‚ÑπÔ∏è  –ê–Ω–∞–ª–∏–∑ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —Ä—ã–Ω–∫–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è—Ö (Wanhua, Covestro, Huntsman).")
        print("‚ÑπÔ∏è  –ó–∞–∫—É–ø–∫–∞ Covestro 44V20L –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ö–∏—Ç–∞–µ.")
        
        news = self.fetch_news(days_back=5)
        print(f"üì• –ü–æ–ª—É—á–µ–Ω–æ {len(news)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        if not news:
            print("‚ùå –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return {
                'analysis_results': [],
                'message': '–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π'
            }
        
        analysis_results = []
        for news_item in news:
            analysis = self.analyze_news_item(news_item)
            analysis_results.append(analysis)
        
        print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analysis_results)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # --- –£–î–ê–õ–ï–ù –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω ---
        
        print("\nüîç –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê (–¢–æ–ø-20 –Ω–æ–≤–æ—Å—Ç–µ–π):")
        for i, result in enumerate(analysis_results[:20], 1): # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π
            news_item = result['news_item']
            print(f"{i}. {news_item['title']}")
            print(f"   –î–∞—Ç–∞: {news_item['date'].strftime('%Y-%m-%d %H:%M')} UTC")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {news_item['source']} [{news_item['category']}]")
            print(f"   –í–ª–∏—è–Ω–∏–µ (Score): {result['total_impact']:.2f}")
            print(f"   –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {result['sentiment']:+.2f}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {news_item.get('relevance_score', 0):.2f}") # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
            if 'matched_keywords' in news_item:
                print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(news_item['matched_keywords'][:10])}") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if 'predicted_impact_date' in news_item:
                print(f"   –í–ª–∏—è–Ω–∏–µ –æ–∂–∏–¥–∞–µ—Ç—Å—è: {news_item['predicted_impact_date'].strftime('%Y-%m-%d')} UTC ({news_item['impact_timing_days']} –¥–Ω–µ–π)")
            print()
        
        # --- –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ ---
        return {
            'analysis_results': analysis_results
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ MDI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # --- –û–¢–õ–ê–î–û–ß–ù–´–ô –í–´–í–û–î –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø ---
    print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
    print(f"–ó–Ω–∞—á–µ–Ω–∏–µ os.environ.get('NEWSAPI_KEY'): {os.environ.get('NEWSAPI_KEY')}")
    # –ü—Ä–æ–≤–µ—Ä–∏–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª .env –∏ –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
    env_path = '.env'
    if os.path.exists(env_path):
        print(f"–§–∞–π–ª {env_path} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        try:
            with open(env_path, 'r') as f:
                content = f.read()
                print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ .env (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {content[:200]}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .env: {e}")
    else:
        print(f"–§–∞–π–ª {env_path} –ù–ï —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
    # ------------------------------------------------------------
    
    enhanced_analyzer = EnhancedMDIAnalyzer()
    
    results = enhanced_analyzer.run_full_analysis()
    
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
    # with open('mdi_news_analysis_results.json', 'w', encoding='utf-8') as f:
    #     json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    # print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ mdi_news_analysis_results.json")