# mdi_forecast_app_advanced.py
import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import os
import warnings
import json
from io import BytesIO
import base64
import nest_asyncio
import asyncio
# –î–ª—è LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import plotly.graph_objects as go
from plotly.subplots import make_subplots
warnings.filterwarnings('ignore')
# –†–∞–∑—Ä–µ—à–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ event loops
nest_asyncio.apply()
# --- –ò–º–ø–æ—Ä—Ç –≤–∞—à–µ–≥–æ –º–æ–¥—É–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π ---
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª news_rss_api_upd5.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ
try:
    from news_rss_api_upd5 import EnhancedMDIAnalyzer
except ImportError as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å EnhancedMDIAnalyzer. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª 'news_rss_api_upd5.py' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –æ–¥–Ω–æ–π –ø–∞–ø–∫–µ —Å —ç—Ç–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º. –û—à–∏–±–∫–∞: {e}")
    st.stop()
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π ---
async def run_news_analysis_async(days_back=5):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π."""
    try:
        analyzer = EnhancedMDIAnalyzer()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º
        if asyncio.iscoroutinefunction(analyzer.news_integrator.get_news_with_timeline):
            results = await analyzer.news_integrator.get_news_with_timeline(hours=days_back*24)
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –≤—ã–∑—ã–≤–∞–µ–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
            results = analyzer.news_integrator.get_news_with_timeline(hours=days_back*24)
        analysis_results = []
        for news_item in results:
            analysis = analyzer.analyze_news_item(news_item)
            analysis_results.append(analysis)
        return {'analysis_results': analysis_results}
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return {'analysis_results': [], 'error': str(e)}
def run_news_analysis(days_back=5):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–Ω—ã–π event loop
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, —Å–æ–∑–¥–∞–µ–º task
                task = loop.create_task(run_news_analysis_async(days_back))
                # –í Streamlit –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å run_until_complete —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ loop –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω
                # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                return {'analysis_results': []}
            else:
                results = loop.run_until_complete(run_news_analysis_async(days_back))
                return results
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(run_news_analysis_async(days_back))
            return results
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return {'analysis_results': [], 'error': str(e)}
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞–∫—Ü–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π ---
@st.cache_data(ttl=3600) # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
def get_producer_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –∞–∫—Ü–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π MDI."""
    tickers = {
        "BAS.DE": "BASF",
        "1COV.DE": "Covestro", 
        "600309.SS": "Wanhua",
        "HUN": "Huntsman"  # –î–æ–±–∞–≤–ª—è–µ–º Huntsman
    }
    rows = []
    log_messages = []
    for tkr, name in tickers.items():
        try:
            df = yf.Ticker(tkr).history(period="max")
            if not df.empty:
                df = df[["Close"]].rename(columns={"Close": "Price"})
                df.index.name = "Date"
                df = df.reset_index()
                df["Date"] = pd.to_datetime(df["Date"]).dt.tz_localize(None).dt.date
                df["Producer"] = name
                rows.append(df)
        except Exception as e:
            error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {name} ({tkr}): {e}"
            log_messages.append(error_msg)
    result = pd.concat(rows, ignore_index=True).sort_values(["Date", "Producer"]) if rows else pd.DataFrame(columns=["Date", "Price", "Producer"])
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    st.session_state.log_messages.extend(log_messages)
    return result
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—ã—Ä—å—é ---
@st.cache_data(ttl=3600) # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
def get_raw_material_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Ü–µ–Ω–∞–º –Ω–∞ —Å—ã—Ä—å–µ."""
    tickers = {
        "Aniline": "BZ=F",   # Brent crude - –ø—Ä–æ–∫—Å–∏ –¥–ª—è –±–µ–Ω–∑–æ–ª–∞/–∞–Ω–∏–ª–∏–Ω–∞
        "NaturalGas": "NG=F"  # Henry Hub
    }
    rows = []
    log_messages = []
    for comp, ticker in tickers.items():
        try:
            df = yf.Ticker(ticker).history(period="max")
            if not df.empty:
                df = df[["Close"]].rename(columns={"Close": "Price"})
                df.index.name = "Date"
                df = df.reset_index()
                df["Date"] = pd.to_datetime(df["Date"]).dt.tz_localize(None).dt.date
                df["Component"] = comp
                rows.append(df)
        except Exception as e:
            error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {comp} ({ticker}): {e}"
            log_messages.append(error_msg)
    result = pd.concat(rows, ignore_index=True).sort_values(["Date", "Component"]) if rows else pd.DataFrame(columns=["Date", "Price", "Component"])
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    st.session_state.log_messages.extend(log_messages)
    return result
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ —Ü–µ–Ω–∞—Ö –Ω–∞ MDI –∏–∑ Excel ---
# –í–ê–ñ–ù–û: –£–±–∏—Ä–∞–µ–º @st.cache_data, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–ª–∏—Å—å
# @st.cache_data
def load_mdi_price_data(file_path=None):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –Ω–∞ MDI –∏–∑ Excel —Ñ–∞–π–ª–∞."""
    log_messages = []
    if file_path is None:
        log_messages.append("–§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ MDI –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –ª–æ–≥–∏, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–µ –æ—à–∏–±–∫–∞
        return pd.DataFrame()
    else:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
        try:
            df = pd.read_excel(file_path)
            log_messages.append(f"Excel —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫.")
            if 'Data Date' in df.columns:
                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                df = df.rename(columns={'Data Date': 'Date'})
                log_messages.append("–°—Ç–æ–ª–±–µ—Ü 'Data Date' —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'Date'.")
            if 'Date' in df.columns:
                initial_count = len(df)
                log_messages.append(f"–ù–∞—á–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å –¥–∞—Ç–∞–º–∏: {initial_count}")
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                original_dates = df['Date'].copy()
                # –§–æ—Ä–º–∞—Ç dd.mm.yyyy (–Ω–∞–ø—Ä–∏–º–µ—Ä, 30.06.2025)
                df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y', errors='coerce')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ –¥–∞—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–ª–æ—Å—å
                successfully_parsed = df['Date'].notna().sum()
                log_messages.append(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –¥–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ dd.mm.yyyy: {successfully_parsed}")
                # –ï—Å–ª–∏ –Ω–µ –≤—Å–µ –¥–∞—Ç—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
                if successfully_parsed < initial_count:
                    log_messages.append("–ù–µ –≤—Å–µ –¥–∞—Ç—ã –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ dd.mm.yyyy. –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã...")
                    unparsed_mask = df['Date'].isna()
                    unparsed_dates = original_dates[unparsed_mask]
                    log_messages.append(f"–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞—Ç—ã: {unparsed_dates.tolist()[:10]}...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    # –ü—Ä–æ–±—É–µ–º —Ñ–æ—Ä–º–∞—Ç dd-mmm-yy (–Ω–∞–ø—Ä–∏–º–µ—Ä, 23-Jun-25)
                    df.loc[unparsed_mask, 'Date'] = pd.to_datetime(unparsed_dates, format='%d-%b-%y', errors='coerce')
                    new_successfully_parsed = df['Date'].notna().sum()
                    log_messages.append(f"–ü–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ dd-mmm-yy —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ: {new_successfully_parsed}")
                    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                    if new_successfully_parsed < initial_count:
                        remaining_unparsed_mask = df['Date'].isna()
                        remaining_unparsed_dates = original_dates[remaining_unparsed_mask]
                        log_messages.append(f"–û—Å—Ç–∞–ª–æ—Å—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞—Ç: {remaining_unparsed_dates.count()}. –ü—Ä–æ–±—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ...")
                        df.loc[remaining_unparsed_mask, 'Date'] = pd.to_datetime(remaining_unparsed_dates, infer_datetime_format=True, errors='coerce')
                        final_successfully_parsed = df['Date'].notna().sum()
                        log_messages.append(f"–ü–æ—Å–ª–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ: {final_successfully_parsed}")
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ date
                df['Date'] = df['Date'].dt.date
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ –¥–∞—Ç—ã
                if df['Date'].isna().all():
                    log_messages.append("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω—É –¥–∞—Ç—É –∏–∑ —Ñ–∞–π–ª–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç –≤ Excel —Ñ–∞–π–ª–µ.")
                    return pd.DataFrame()
                else:
                    successfully_converted_to_date = df['Date'].notna().sum()
                    log_messages.append(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ –æ–±—ä–µ–∫—Ç—ã date: {successfully_converted_to_date}")
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                numeric_columns = ['Lowest', 'Highest', 'Mainstream', 'Chg']
                for col in numeric_columns:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        log_messages.append(f"–°—Ç–æ–ª–±–µ—Ü '{col}' –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç. –ù–µ-—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ NaN.")
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Chg% –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                if 'Chg%' in df.columns:
                    # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª %, –∑–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ
                    df['Chg%'] = df['Chg%'].astype(str).str.rstrip('%').str.replace(',', '.').astype('float') / 100.0
                    log_messages.append("–°—Ç–æ–ª–±–µ—Ü 'Chg%' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
                if df['Date'].notna().any():
                    min_date = df['Date'].min()
                    max_date = df['Date'].max()
                    log_messages.append(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –≤ —Ñ–∞–π–ª–µ: —Å {min_date} –ø–æ {max_date}")
                else:
                    log_messages.append("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç.")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
                if 'log_messages' not in st.session_state:
                    st.session_state.log_messages = []
                st.session_state.log_messages.extend(log_messages)
                return df
            else:
                log_messages.append("–û—à–∏–±–∫–∞: –í Excel —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'Date' –∏–ª–∏ 'Data Date'.")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
                if 'log_messages' not in st.session_state:
                    st.session_state.log_messages = []
                st.session_state.log_messages.extend(log_messages)
                return pd.DataFrame()
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}"
            log_messages.append(error_msg)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
            if 'log_messages' not in st.session_state:
                st.session_state.log_messages = []
            st.session_state.log_messages.extend(log_messages)
            import traceback
            st.session_state.log_messages.append(traceback.format_exc())
            return pd.DataFrame()
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ ---
def prepare_forecast_data(mdi_df, producer_df, raw_material_df, news_results):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞, –æ–±—ä–µ–¥–∏–Ω—è—è –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."""
    log_messages = []
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å 01.09.2024
    start_date = datetime(2024, 9, 1).date()
    # –§–∏–ª—å—Ç—Ä—É–µ–º MDI –¥–∞–Ω–Ω—ã–µ
    if not mdi_df.empty:
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å—Ç–æ–ª–±–µ—Ü Date —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
        if 'Date' in mdi_df.columns:
            mdi_df = mdi_df.copy()  # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —á—Ç–æ–±—ã –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π DataFrame
            # mdi_df['Date'] = pd.to_datetime(mdi_df['Date'], errors='coerce').dt.date
            # –£–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ load_mdi_price_data
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –Ω–∞—á–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã
            total_rows_before_filter = len(mdi_df)
            mdi_df = mdi_df[mdi_df['Date'] >= start_date]
            rows_after_filter = len(mdi_df)
            log_messages.append(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è MDI –¥–∞–Ω–Ω—ã—Ö —Å {start_date}: –±—ã–ª–æ {total_rows_before_filter} —Å—Ç—Ä–æ–∫, –æ—Å—Ç–∞–ª–æ—Å—å {rows_after_filter} —Å—Ç—Ä–æ–∫.")
            if mdi_df.empty:
                log_messages.append(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å {start_date} –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö MDI. –í–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ –¥–∞—Ç—ã –≤ —Ñ–∞–π–ª–µ —Ä–∞–Ω—å—à–µ —ç—Ç–æ–π –¥–∞—Ç—ã.")
        else:
            log_messages.append("–í –¥–∞–Ω–Ω—ã—Ö MDI –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'Date'")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
            if 'log_messages' not in st.session_state:
                st.session_state.log_messages = []
            st.session_state.log_messages.extend(log_messages)
            return pd.DataFrame()
    else:
        log_messages.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö MDI")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []
        st.session_state.log_messages.extend(log_messages)
        return pd.DataFrame()
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π DataFrame —Å –¥–∞—Ç–∞–º–∏
    if mdi_df.empty:
        log_messages.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö MDI –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –ø–µ—Ä–∏–æ–¥–µ")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []
        st.session_state.log_messages.extend(log_messages)
        return pd.DataFrame()
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö MDI
    actual_start_date = mdi_df['Date'].min()
    actual_end_date = mdi_df['Date'].max()
    all_dates = pd.date_range(start=actual_start_date, end=actual_end_date, freq='D')
    all_dates = [d.date() for d in all_dates]
    df = pd.DataFrame({'Date': all_dates})
    log_messages.append(f"–°–æ–∑–¥–∞–Ω –±–∞–∑–æ–≤—ã–π DataFrame —Å –¥–∞—Ç–∞–º–∏ –æ—Ç {actual_start_date} –¥–æ {actual_end_date} ({len(all_dates)} –¥–Ω–µ–π).")
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –Ω–∞ MDI
    if not mdi_df.empty and 'Mainstream' in mdi_df.columns:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ª–±–µ—Ü Mainstream –¥–ª—è —Ü–µ–Ω—ã MDI
        mdi_prices = mdi_df[['Date', 'Mainstream']].rename(columns={'Mainstream': 'MDI_Price'})
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –¥–∞—Ç–µ, –æ—Å—Ç–∞–≤–ª—è—è –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        initial_mdiprices_len = len(mdi_prices)
        mdi_prices = mdi_prices.drop_duplicates(subset=['Date'], keep='last')
        final_mdiprices_len = len(mdi_prices)
        if initial_mdiprices_len != final_mdiprices_len:
            log_messages.append(f"–£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö MDI: –±—ã–ª–æ {initial_mdiprices_len}, –æ—Å—Ç–∞–ª–æ—Å—å {final_mdiprices_len}.")
        df = df.merge(mdi_prices, on='Date', how='left')
        merged_count = df['MDI_Price'].notna().sum()
        log_messages.append(f"–¶–µ–Ω—ã MDI –¥–æ–±–∞–≤–ª–µ–Ω—ã. –ù–∞–π–¥–µ–Ω–æ {merged_count} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –¥–∞—Ç–∞–º.")
    else:
        log_messages.append("–í –¥–∞–Ω–Ω—ã—Ö MDI –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'Mainstream'")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
        if 'log_messages' not in st.session_state:
            st.session_state.log_messages = []
        st.session_state.log_messages.extend(log_messages)
        return pd.DataFrame()
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –Ω–∞ –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
    producers_added = 0
    if not producer_df.empty:
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–∞–º –∏–∑ MDI
        producer_df_filtered = producer_df[(producer_df['Date'] >= actual_start_date) & (producer_df['Date'] <= actual_end_date)].copy()
        if not producer_df_filtered.empty:
            producer_df_filtered['Date'] = pd.to_datetime(producer_df_filtered['Date']).dt.date
            producer_pivot = producer_df_filtered.pivot(index='Date', columns='Producer', values='Price')
            producer_pivot = producer_pivot.fillna(method='ffill').fillna(method='bfill').fillna(0)
            df = df.merge(producer_pivot, left_on='Date', right_index=True, how='left')
            producers_added = producer_pivot.shape[1] # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            log_messages.append(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø–æ –∞–∫—Ü–∏—è–º {producers_added} –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π.")
        else:
             log_messages.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞–∫—Ü–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.")
    else:
        log_messages.append("–ù–µ—Ç –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞–∫—Ü–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π.")
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ
    materials_added = 0
    if not raw_material_df.empty:
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–∞–º –∏–∑ MDI
        raw_material_df_filtered = raw_material_df[(raw_material_df['Date'] >= actual_start_date) & (raw_material_df['Date'] <= actual_end_date)].copy()
        if not raw_material_df_filtered.empty:
            raw_material_df_filtered['Date'] = pd.to_datetime(raw_material_df_filtered['Date']).dt.date
            raw_material_pivot = raw_material_df_filtered.pivot(index='Date', columns='Component', values='Price')
            raw_material_pivot = raw_material_pivot.fillna(method='ffill').fillna(method='bfill').fillna(0)
            df = df.merge(raw_material_pivot, left_on='Date', right_index=True, how='left')
            materials_added = raw_material_pivot.shape[1] # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            log_messages.append(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø–æ —Ü–µ–Ω–∞–º –Ω–∞ {materials_added} –≤–∏–¥–æ–≤ —Å—ã—Ä—å—è.")
        else:
             log_messages.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ü–µ–Ω–∞–º –Ω–∞ —Å—ã—Ä—å–µ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.")
    else:
        log_messages.append("–ù–µ—Ç –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ü–µ–Ω–∞–º –Ω–∞ —Å—ã—Ä—å–µ.")
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è —Ñ–ª–∞–≥–∞ "–Ω–æ–≤–æ—Å—Ç—å"
    df['News_Event_Score'] = 0.0
    news_added = 0
    if news_results and 'analysis_results' in news_results:
        for result in news_results['analysis_results']:
            try:
                if 'news_item' in result and 'date' in result['news_item']:
                    news_date = result['news_item']['date'].date()
                    if actual_start_date <= news_date <= actual_end_date:
                        impact_score = result.get('total_impact', 0) * result.get('sentiment', 0)
                        for i in range(5):  # –í–ª–∏—è–Ω–∏–µ –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –¥–Ω–µ–π
                            target_date = news_date + timedelta(days=i)
                            if target_date <= actual_end_date:
                                df.loc[df['Date'] == target_date, 'News_Event_Score'] += impact_score * (0.8 ** i)
                                news_added += 1
            except Exception as e:
                log_messages.append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
        if news_added > 0:
            log_messages.append(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {news_added} –∑–∞–ø–∏—Å–µ–π –æ –Ω–æ–≤–æ—Å—Ç—è—Ö.")
        else:
             log_messages.append("–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç).")
    else:
        log_messages.append("–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞).")
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª—è–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    initial_na_counts = df[numeric_columns].isna().sum()
    df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(method='bfill').fillna(0)
    final_na_counts = df[numeric_columns].isna().sum()
    filled_any = (initial_na_counts > final_na_counts).any()
    if filled_any:
        log_messages.append("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö –∑–∞–ø–æ–ª–Ω–µ–Ω—ã.")
    else:
        log_messages.append("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö –Ω–µ –±—ã–ª–æ –∏–ª–∏ –æ–Ω–∏ —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã.")
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ü–µ–Ω –Ω–∞ MDI
    initial_len = len(df)
    if 'MDI_Price' in df.columns:
        df = df.dropna(subset=['MDI_Price'])
        final_len = len(df)
        if initial_len != final_len:
            log_messages.append(f"–£–¥–∞–ª–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ü–µ–Ω—ã MDI: –±—ã–ª–æ {initial_len}, –æ—Å—Ç–∞–ª–æ—Å—å {final_len}.")
        else:
            log_messages.append(f"–í—Å–µ {final_len} —Å—Ç—Ä–æ–∫ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ü–µ–Ω—ã MDI.")
    else:
        log_messages.append("–°—Ç–æ–ª–±–µ—Ü 'MDI_Price' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∏—Ç–æ–≥–æ–≤–æ–º DataFrame.")
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    st.session_state.log_messages.extend(log_messages)
    st.success(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤—ã–π DataFrame —Å–æ–¥–µ—Ä–∂–∏—Ç {len(df)} —Å—Ç—Ä–æ–∫ –∏ {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤.")
    return df
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM ---
def create_dataset(dataset, look_back=1):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å—Å–∏–≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –≤ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM."""
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), :]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ MDI_Price
    return np.array(dataX), np.array(dataY)
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LSTM ---
def forecast_mdi_price_lstm(df, look_back=10, epochs=50, batch_size=1, neurons=50, dropout=0.2, train_split=0.8):
    """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –Ω–∞ MDI —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LSTM."""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    if df.empty:
        st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
        return None, None, None, None, None
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    features = ['MDI_Price']
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
    producer_cols = ['BASF', 'Covestro', 'Wanhua', 'Huntsman']
    for col in producer_cols:
        if col in df.columns:
            features.append(col)
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã —Å—ã—Ä—å—è
    raw_material_cols = ['Aniline', 'NaturalGas']
    for col in raw_material_cols:
        if col in df.columns:
            features.append(col)
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü –Ω–æ–≤–æ—Å—Ç–µ–π –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if 'News_Event_Score' in df.columns:
        features.append('News_Event_Score')
    if len(features) == 0:
        st.error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
        return None, None, None, None, None
    if len(df) < look_back + 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        st.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {look_back + 10} –∑–∞–ø–∏—Å–µ–π, –¥–æ—Å—Ç—É–ø–Ω–æ {len(df)}")
        return None, None, None, None, None
    data = df[features].copy()
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    data = data.fillna(method='ffill').fillna(method='bfill').fillna(0)
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    train_size = max(int(len(scaled_data) * train_split), look_back + 5)  # –ú–∏–Ω–∏–º—É–º look_back + 5 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if train_size >= len(scaled_data) - look_back - 5:
        train_size = len(scaled_data) - look_back - 5
    if train_size <= look_back + 1:
        st.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤")
        return None, None, None, None, None
    train, test = scaled_data[0:train_size,:], scaled_data[train_size:len(scaled_data),:]
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    trainX, trainY = create_dataset(train, look_back)
    testX, testY = create_dataset(test, look_back)
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø—É—Å—Ç—ã–µ
    if len(trainX) == 0:
        st.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞")
        return None, None, None, None, None
    if len(testX) == 0:
        st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä.")
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
        if len(trainX) > 1:
            testX, testY = trainX[-1:], trainY[-1:]
            trainX, trainY = trainX[:-1], trainY[:-1]
        else:
            st.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã")
            return None, None, None, None, None
    # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM [samples, time steps, features]
    try:
        trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], len(features)))
        testX = np.reshape(testX, (testX.shape[0], testX.shape[1], len(features)))
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None, None, None, None, None
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM
    model = Sequential()
    model.add(LSTM(neurons, return_sequences=True, input_shape=(look_back, len(features))))
    model.add(Dropout(dropout))
    model.add(LSTM(neurons, return_sequences=False))
    model.add(Dropout(dropout))
    model.add(Dense(25))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    try:
        model.fit(trainX, trainY, validation_data=(testX, testY), epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None, None, None, None
    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
    try:
        trainPredict = model.predict(trainX)
        testPredict = model.predict(testX)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None, None, None, None, None
    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    trainPredict_extended = np.zeros((len(trainPredict), len(features)))
    trainPredict_extended[:,0] = trainPredict[:,0]
    trainPredict = scaler.inverse_transform(trainPredict_extended)[:,0]
    testPredict_extended = np.zeros((len(testPredict), len(features)))
    testPredict_extended[:,0] = testPredict[:,0]
    testPredict = scaler.inverse_transform(testPredict_extended)[:,0]
    trainY_extended = np.zeros((len(trainY), len(features)))
    trainY_extended[:,0] = trainY
    trainY = scaler.inverse_transform(trainY_extended)[:,0]
    testY_extended = np.zeros((len(testY), len(features)))
    testY_extended[:,0] = testY
    testY = scaler.inverse_transform(testY_extended)[:,0]
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict)) if len(trainY) > 0 else 0
    testScore = np.sqrt(mean_squared_error(testY, testPredict)) if len(testY) > 0 else 0
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_df = df.copy()
    results_df['Predicted_Price'] = np.nan
    results_df['Train/Test'] = ''
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if len(trainPredict) > 0:
        train_dates = df.iloc[look_back:len(trainPredict)+look_back]['Date']
        results_df.loc[results_df['Date'].isin(train_dates), 'Predicted_Price'] = trainPredict
        results_df.loc[results_df['Date'].isin(train_dates), 'Train/Test'] = 'Train'
    if len(testPredict) > 0:
        test_start_idx = len(trainPredict) + (look_back*2) + 1
        test_end_idx = test_start_idx + len(testPredict)
        if test_end_idx <= len(df):
            test_dates = df.iloc[test_start_idx:test_end_idx]['Date']
            results_df.loc[results_df['Date'].isin(test_dates), 'Predicted_Price'] = testPredict
            results_df.loc[results_df['Date'].isin(test_dates), 'Train/Test'] = 'Test'
    return results_df, trainScore, testScore, model, scaler
# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –±—É–¥—É—â–µ–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ---
def forecast_future(model, scaler, last_sequence, n_future_days, features):
    """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Ü–µ–Ω—ã –Ω–∞ –±—É–¥—É—â–∏–µ –¥–Ω–∏."""
    future_predictions = []
    current_batch = last_sequence.copy()
    for _ in range(n_future_days):
        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        current_batch_reshaped = current_batch.reshape((1, current_batch.shape[0], current_batch.shape[1]))
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
        next_pred = model.predict(current_batch_reshaped, verbose=0)
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Å–ø–∏—Å–æ–∫
        future_predictions.append(next_pred[0, 0])
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        new_row = current_batch[-1].copy()  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É
        new_row[0] = next_pred[0, 0]  # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ MDI_Price
        current_batch = np.append(current_batch[1:], [new_row], axis=0)  # –°–¥–≤–∏–≥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    future_predictions_array = np.array(future_predictions)
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    temp_array = np.zeros((len(future_predictions_array), len(features)))
    temp_array[:, 0] = future_predictions_array
    future_predictions_inv = scaler.inverse_transform(temp_array)[:, 0]
    return future_predictions_inv
# --- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit ---
def main():
    st.set_page_config(page_title="–ü—Ä–æ–≥–Ω–æ–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ MDI (–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)", layout="wide")
    st.title("üìä –ü—Ä–æ–≥–Ω–æ–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ MDI (–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)")
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state –¥–ª—è –ª–æ–≥–æ–≤
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    # --- –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å ---
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    st.sidebar.subheader("–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
    days_back = st.sidebar.slider("–ü–µ—Ä–∏–æ–¥ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–¥–Ω–∏)", min_value=1, max_value=14, value=5, step=1)
    run_news_button = st.sidebar.button("–û–±–Ω–æ–≤–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")
    upload_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏ –Ω–∞ MDI", type=["xlsx", "xls"])
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
    st.sidebar.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ LSTM")
    look_back = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (look_back)", min_value=5, max_value=30, value=10, step=1)
    epochs = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è", min_value=10, max_value=200, value=50, step=10)
    batch_size = st.sidebar.selectbox("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞", options=[1, 16, 32, 64], index=0)
    neurons = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–ª–æ–µ", min_value=20, max_value=200, value=50, step=10)
    dropout = st.sidebar.slider("Dropout (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)", min_value=0.0, max_value=0.5, value=0.2, step=0.05)
    train_split = st.sidebar.slider("–î–æ–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏", min_value=0.5, max_value=0.95, value=0.8, step=0.05)
    forecast_days = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞", min_value=10, max_value=180, value=60, step=10)
    # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
    run_forecast_button = st.sidebar.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑")
    # --- –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---
    # –í–ê–ñ–ù–û: load_all_data –Ω–µ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–ª–∏—Å—å
    # @st.cache_data(show_spinner=False)
    def load_all_data():
        with st.spinner('–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...'):
            producer_data = get_producer_data()
            raw_material_data = get_raw_material_data()
        # –í–ê–ñ–ù–û: load_mdi_price_data –ù–ï –∫—ç—à–∏—Ä—É–µ—Ç—Å—è
        mdi_data = load_mdi_price_data(upload_file)
        return producer_data, raw_material_data, mdi_data
    producer_data, raw_material_data, mdi_data = load_all_data()
    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è ---
    st.sidebar.subheader("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–ó–∞–≥—Ä—É–∑–∫–∞")
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    state_to_save = {
        "producer_data": producer_data.to_dict('records') if not producer_data.empty else [],
        "raw_material_data": raw_material_data.to_dict('records') if not raw_material_data.empty else [],
        "mdi_data": mdi_data.to_dict('records') if not mdi_data.empty else [],
        "last_run_params": {
            "days_back": days_back,
            "look_back": look_back,
            "epochs": epochs,
            "batch_size": batch_size,
            "neurons": neurons,
            "dropout": dropout,
            "train_split": train_split,
            "forecast_days": forecast_days
        },
        "log_messages": st.session_state.log_messages
    }
    state_json = json.dumps(state_to_save, default=str)
    b64 = base64.b64encode(state_json.encode()).decode()
    href = f'<a href="file/json;base64,{b64}" download="mdi_forecast_state.json">–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ</a>'
    st.sidebar.markdown(href, unsafe_allow_html=True)
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    uploaded_state = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ", type=["json"])
    if uploaded_state is not None:
        try:
            state_data = json.load(uploaded_state)
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            producer_data = pd.DataFrame(state_data.get("producer_data", []))
            raw_material_data = pd.DataFrame(state_data.get("raw_material_data", []))
            mdi_data = pd.DataFrame(state_data.get("mdi_data", []))
            st.session_state.log_messages = state_data.get("log_messages", [])
            st.sidebar.success("–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ!")
            st.experimental_rerun()  # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        except Exception as e:
            st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    # --- –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π ---
    if 'news_results' not in st.session_state:
        st.session_state.news_results = None
    if run_news_button:
        with st.spinner('–ò–¥–µ—Ç —Å–±–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π...'):
            st.session_state.news_results = run_news_analysis(days_back=days_back)
            if st.session_state.news_results and 'analysis_results' in st.session_state.news_results and len(st.session_state.news_results['analysis_results']) > 0:
                st.sidebar.success(f"–ù–∞–π–¥–µ–Ω–æ {len(st.session_state.news_results['analysis_results'])} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.")
            else:
                st.sidebar.warning("–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ ---
    combined_data = pd.DataFrame()
    if not mdi_data.empty:
        with st.spinner('–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞...'):
            combined_data = prepare_forecast_data(mdi_data, producer_data, raw_material_data, st.session_state.news_results)
            if not combined_data.empty:
                st.success(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(combined_data)}")
            else:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
    # --- –í–∫–ª–∞–¥–∫–∏ ---
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["–î–∞–Ω–Ω—ã–µ", "–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π", "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã", "–ü—Ä–æ–≥–Ω–æ–∑", "–ú–æ–¥–µ–ª—å", "Logs"])
    with tab1:
        st.subheader("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö MDI –∏ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å 01.09.2024
        start_date = datetime(2024, 9, 1).date()
        # –§–∏–ª—å—Ç—Ä—É–µ–º MDI –¥–∞–Ω–Ω—ã–µ
        if not mdi_data.empty:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ Date —Å—Ç–æ–ª–±–µ—Ü –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
            mdi_data_copy = mdi_data.copy()
            if 'Date' in mdi_data_copy.columns:
                # mdi_data_copy['Date'] = pd.to_datetime(mdi_data_copy['Date'], errors='coerce').dt.date
                # –£–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ load_mdi_price_data
                mdi_filtered = mdi_data_copy[mdi_data_copy['Date'] >= start_date]
            else:
                mdi_filtered = pd.DataFrame()
        else:
            mdi_filtered = pd.DataFrame()
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∞–∫—Ü–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
        if not producer_data.empty:
            producer_data_copy = producer_data.copy()
            if 'Date' in producer_data_copy.columns:
                producer_data_copy['Date'] = pd.to_datetime(producer_data_copy['Date'], errors='coerce').dt.date
                producer_filtered = producer_data_copy[producer_data_copy['Date'] >= start_date]
            else:
                producer_filtered = pd.DataFrame()
        else:
            producer_filtered = pd.DataFrame()
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å—ã—Ä—å—è
        if not raw_material_data.empty:
            raw_material_data_copy = raw_material_data.copy()
            if 'Date' in raw_material_data_copy.columns:
                raw_material_data_copy['Date'] = pd.to_datetime(raw_material_data_copy['Date'], errors='coerce').dt.date
                raw_material_filtered = raw_material_data_copy[raw_material_data_copy['Date'] >= start_date]
            else:
                raw_material_filtered = pd.DataFrame()
        else:
            raw_material_filtered = pd.DataFrame()
        if not mdi_filtered.empty:
            # –°–æ–∑–¥–∞–µ–º –¥–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –æ–¥–∏–Ω –ø–æ–¥ –¥—Ä—É–≥–∏–º
            # –ü–µ—Ä–≤—ã–π –≥—Ä–∞—Ñ–∏–∫: –¶–µ–Ω–∞ MDI –∏ –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
            st.subheader("–¶–µ–Ω–∞ MDI –∏ –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π")
            fig_mdi_producers = go.Figure()
            # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é MDI Price
            fig_mdi_producers.add_trace(
                go.Scatter(
                    x=mdi_filtered['Date'], 
                    y=mdi_filtered['Mainstream'], 
                    mode='lines+markers', 
                    name='MDI Price (Mainstream)',
                    line=dict(color='red', width=2),
                    yaxis='y1'
                )
            )
            # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –Ω–∞ –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
            available_producers = ['BASF', 'Covestro', 'Wanhua', 'Huntsman']
            colors = ['blue', 'green', 'orange', 'purple']
            if not producer_filtered.empty:
                for i, producer in enumerate(available_producers):
                    if producer in producer_filtered['Producer'].values:
                        producer_stock_data = producer_filtered[producer_filtered['Producer'] == producer]
                        fig_mdi_producers.add_trace(
                            go.Scatter(
                                x=producer_stock_data['Date'], 
                                y=producer_stock_data['Price'], 
                                mode='lines', 
                                name=f'{producer} (–ê–∫—Ü–∏–∏)',
                                line=dict(color=colors[i % len(colors)], width=1, dash='dot'),
                                yaxis='y2'
                            )
                        )
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
            fig_mdi_producers.update_layout(
                title="–¶–µ–Ω–∞ MDI –∏ –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π",
                xaxis_title="–î–∞—Ç–∞",
                yaxis=dict(
                    title="–¶–µ–Ω–∞ MDI (Yuan/mt)",
                ),
                yaxis2=dict(
                    title="–¶–µ–Ω–∞ –∞–∫—Ü–∏–π (USD/EUR/CNY)",
                    anchor="x",
                    overlaying="y",
                    side="right"
                ),
                hovermode='x unified',
                height=500
            )
            # –ó–∞–¥–∞–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –æ—Å–µ–π (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
            fig_mdi_producers.update_yaxes(title_font_color="red", tickfont_color="red", selector=dict(name="y"))
            fig_mdi_producers.update_yaxes(title_font_color="blue", tickfont_color="blue", selector=dict(name="y2"))
            st.plotly_chart(fig_mdi_producers, use_container_width=True)
            # –í—Ç–æ—Ä–æ–π –≥—Ä–∞—Ñ–∏–∫: –¶–µ–Ω–∞ MDI –∏ —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ
            st.subheader("–¶–µ–Ω–∞ MDI –∏ —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ")
            fig_mdi_raw = go.Figure()
            # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é MDI Price (–ø–æ–≤—Ç–æ—Ä–Ω–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
            fig_mdi_raw.add_trace(
                go.Scatter(
                    x=mdi_filtered['Date'], 
                    y=mdi_filtered['Mainstream'], 
                    mode='lines+markers', 
                    name='MDI Price (Mainstream) - –°—ã—Ä—å–µ',
                    line=dict(color='red', width=2),
                    yaxis='y1'
                )
            )
            # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ
            available_materials = ['Aniline', 'NaturalGas']
            material_colors = ['brown', 'pink']
            if not raw_material_filtered.empty:
                for i, material in enumerate(available_materials):
                    if material in raw_material_filtered['Component'].values:
                        material_data = raw_material_filtered[raw_material_filtered['Component'] == material]
                        fig_mdi_raw.add_trace(
                            go.Scatter(
                                x=material_data['Date'], 
                                y=material_data['Price'], 
                                mode='lines', 
                                name=f'{material} (–°—ã—Ä—å–µ)',
                                line=dict(color=material_colors[i % len(material_colors)], width=1, dash='dot'),
                                yaxis='y2'
                            )
                        )
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
            fig_mdi_raw.update_layout(
                title="–¶–µ–Ω–∞ MDI –∏ —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ",
                xaxis_title="–î–∞—Ç–∞",
                yaxis=dict(
                    title="–¶–µ–Ω–∞ MDI (Yuan/mt)",
                ),
                yaxis2=dict(
                    title="–¶–µ–Ω–∞ —Å—ã—Ä—å—è (USD)",
                    anchor="x",
                    overlaying="y",
                    side="right"
                ),
                hovermode='x unified',
                height=500
            )
            # –ó–∞–¥–∞–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –æ—Å–µ–π (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
            fig_mdi_raw.update_yaxes(title_font_color="red", tickfont_color="red", selector=dict(name="y"))
            fig_mdi_raw.update_yaxes(title_font_color="brown", tickfont_color="brown", selector=dict(name="y2"))
            st.plotly_chart(fig_mdi_raw, use_container_width=True)
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã
            st.subheader("–ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ")
            # –¢–∞–±–ª–∏—Ü–∞ —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ —Ü–µ–Ω–∞–º–∏ MDI
            st.write("**–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã MDI**")
            # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            mdi_display_cols = ['Date', 'Mainstream', 'Lowest', 'Highest', 'Chg']
            if 'Chg%' in mdi_filtered.columns:
                mdi_display_cols.append('Chg%')
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–æ–ª–±—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            existing_mdi_cols = [col for col in mdi_display_cols if col in mdi_filtered.columns]
            if existing_mdi_cols:
                st.dataframe(mdi_filtered[existing_mdi_cols].head(10), use_container_width=True)
            else:
                st.write("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
        else:
            if upload_file is None:
                st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ü–µ–Ω–∞–º MDI.")
            else:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∏ –¥–∞—Ç—ã.")
        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö MDI
        if not mdi_filtered.empty:
            csv = mdi_filtered.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ MDI (CSV)",
                data=csv,
                file_name='mdi_historical_data.csv',
                mime='text/csv',
            )
    with tab2:
        st.subheader("–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π")
        if st.session_state.news_results and 'analysis_results' in st.session_state.news_results and len(st.session_state.news_results['analysis_results']) > 0:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
            news_df = []
            for result in st.session_state.news_results['analysis_results']:
                news_item = result['news_item']
                news_row = {
                    'Title': news_item.get('title', ''),
                    'Published_Date_UTC': news_item.get('date', '').strftime('%Y-%m-%d %H:%M:%S') if hasattr(news_item.get('date'), 'strftime') else '',
                    'Source': news_item.get('source', ''),
                    'Total_Impact_Score': round(result.get('total_impact', 0), 4),
                    'Sentiment': round(result.get('sentiment', 0), 4),
                    'Relevance_Score': round(news_item.get('relevance_score', 0), 4),
                    'Impact_Timing_Days': news_item.get('impact_timing_days', 0),
                    'URL': news_item.get('url', '')
                }
                news_df.append(news_row)
            df_news = pd.DataFrame(news_df)
            st.dataframe(df_news, use_container_width=True)
            # –ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
            if not df_news.empty:
                st.subheader("–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
                fig_news = make_subplots(specs=[[{"secondary_y": True}]])
                fig_news.add_trace(
                    go.Scatter(x=pd.to_datetime(df_news['Published_Date_UTC']), y=df_news['Total_Impact_Score'], mode='markers', name='–í–ª–∏—è–Ω–∏–µ', marker=dict(size=10, color=df_news['Sentiment'], colorscale='RdYlGn', showscale=True)),
                    secondary_y=False,
                )
                fig_news.update_layout(title="–ù–æ–≤–æ—Å—Ç–∏: –í–ª–∏—è–Ω–∏–µ vs –î–∞—Ç–∞ (–¶–≤–µ—Ç - –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ)", xaxis_title="–î–∞—Ç–∞", yaxis_title="–í–ª–∏—è–Ω–∏–µ")
                st.plotly_chart(fig_news, use_container_width=True)
        else:
            st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–û–±–Ω–æ–≤–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏' –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞.")
    with tab3:
        st.subheader("–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –Ω–∞ MDI")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–∞
        start_date = datetime(2024, 9, 1).date()
        if not mdi_data.empty:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å 01.09.2024
            mdi_data_copy = mdi_data.copy()
            if 'Date' in mdi_data_copy.columns:
                # mdi_data_copy['Date'] = pd.to_datetime(mdi_data_copy['Date'], errors='coerce').dt.date
                # –£–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ
                mdi_filtered = mdi_data_copy[mdi_data_copy['Date'] >= start_date]
                if not mdi_filtered.empty:
                    fig_mdi_history = go.Figure()
                    fig_mdi_history.add_trace(go.Scatter(x=mdi_filtered['Date'], y=mdi_filtered['Mainstream'], mode='lines+markers', name='Mainstream Price', line=dict(color='red')))
                    fig_mdi_history.update_layout(title="–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –Ω–∞ PolyMDI", xaxis_title="–î–∞—Ç–∞", yaxis_title="–¶–µ–Ω–∞ (Yuan/mt)", hovermode='x')
                    st.plotly_chart(fig_mdi_history, use_container_width=True)
                    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–∞–Ω–Ω—ã—Ö
                    st.write("**–¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω**")
                    display_columns = ['Date', 'Mainstream', 'Lowest', 'Highest', 'Chg']
                    if 'Chg%' in mdi_filtered.columns:
                        display_columns.append('Chg%')
                    available_columns = [col for col in display_columns if col in mdi_filtered.columns]
                    st.dataframe(mdi_filtered[available_columns])
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω —Å 01.09.2024")
            else:
                st.warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö MDI")
        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ü–µ–Ω–∞–º MDI")
    with tab4:
        st.subheader("–ü—Ä–æ–≥–Ω–æ–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ MDI")
        if not combined_data.empty and run_forecast_button:
            with st.spinner('–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ...'):
                try:
                    results_df, train_rmse, test_rmse, model, scaler = forecast_mdi_price_lstm(
                        combined_data, look_back, epochs, batch_size, neurons, dropout, train_split
                    )
                    if results_df is not None and not results_df.empty:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                        features = ['MDI_Price']
                        producer_cols = ['BASF', 'Covestro', 'Wanhua', 'Huntsman']
                        for col in producer_cols:
                            if col in combined_data.columns:
                                features.append(col)
                        raw_material_cols = ['Aniline', 'NaturalGas']
                        for col in raw_material_cols:
                            if col in combined_data.columns:
                                features.append(col)
                        if 'News_Event_Score' in combined_data.columns:
                            features.append('News_Event_Score')
                        # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –±—É–¥—É—â–µ–µ
                        if len(features) > 0 and len(combined_data) >= look_back:
                            try:
                                last_sequence = scaler.transform(combined_data[features].tail(look_back).fillna(method='ffill').fillna(method='bfill').fillna(0).values)
                                future_dates = [combined_data['Date'].max() + timedelta(days=i) for i in range(1, forecast_days+1)]
                                future_predictions = forecast_future(model, scaler, last_sequence, forecast_days, features)
                                # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
                                future_df = pd.DataFrame({'Date': future_dates, 'Predicted_Price': future_predictions, 'Train/Test': 'Future'})
                                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                                final_results_df = pd.concat([results_df, future_df], ignore_index=True)
                                # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
                                fig_forecast = go.Figure()
                                # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
                                fig_forecast.add_trace(go.Scatter(x=combined_data['Date'], y=combined_data['MDI_Price'], mode='lines', name='–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞', line=dict(color='blue')))
                                # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
                                test_data = final_results_df[final_results_df['Train/Test'] == 'Test']
                                if not test_data.empty:
                                    fig_forecast.add_trace(go.Scatter(x=test_data['Date'], y=test_data['Predicted_Price'], mode='lines', name='–ü—Ä–æ–≥–Ω–æ–∑ (–¢–µ—Å—Ç)', line=dict(color='green', dash='dot')))
                                # –ë—É–¥—É—â–∏–π –ø—Ä–æ–≥–Ω–æ–∑
                                future_data = final_results_df[final_results_df['Train/Test'] == 'Future']
                                if not future_data.empty:
                                    fig_forecast.add_trace(go.Scatter(x=future_data['Date'], y=future_data['Predicted_Price'], mode='lines', name='–ë—É–¥—É—â–∏–π –ø—Ä–æ–≥–Ω–æ–∑', line=dict(color='orange', dash='dash')))
                                fig_forecast.update_layout(
                                    title="–ü—Ä–æ–≥–Ω–æ–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ MDI (LSTM)",
                                    xaxis_title="–î–∞—Ç–∞",
                                    yaxis_title="–¶–µ–Ω–∞ (Yuan/mt)",
                                    hovermode='x unified'
                                )
                                st.plotly_chart(fig_forecast, use_container_width=True)
                                # –ú–µ—Ç—Ä–∏–∫–∏
                                col1, col2 = st.columns(2)
                                if train_rmse is not None:
                                    col1.metric("RMSE (–û–±—É—á–µ–Ω–∏–µ)", f"{train_rmse:.2f}")
                                if test_rmse is not None:
                                    col2.metric("RMSE (–¢–µ—Å—Ç)", f"{test_rmse:.2f}")
                                # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
                                st.write("**–¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞**")
                                st.dataframe(final_results_df[['Date', 'MDI_Price', 'Predicted_Price', 'Train/Test']].tail(20))
                                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                                forecast_csv = final_results_df.to_csv(index=False).encode('utf-8-sig')
                                st.download_button(
                                    label="–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ (CSV)",
                                    data=forecast_csv,
                                    file_name='mdi_forecast_lstm.csv',
                                    mime='text/csv',
                                )
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–∞ –±—É–¥—É—â–µ–µ: {e}")
                        else:
                            st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
                    else:
                        st.error("–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                    st.exception(e)
        elif not run_forecast_button:
            st.info("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑'.")
        else:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω Excel —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
    with tab5:
        st.subheader("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ LSTM")
        st.markdown("""
        **–ß—Ç–æ —Ç–∞–∫–æ–µ LSTM?**
        LSTM (Long Short-Term Memory) - —ç—Ç–æ —Ç–∏–ø —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (RNN), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö. –û–Ω–∞ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –º–æ–∂–µ—Ç "–∑–∞–ø–æ–º–∏–Ω–∞—Ç—å" –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∏—Ö –ø—Ä–∏ –ø—Ä–∏–Ω—è—Ç–∏–∏ —Ä–µ—à–µ–Ω–∏–π –æ –±—É–¥—É—â–µ–º.
        **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ —ç—Ç–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏:**
        1. **–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –Ω–∞ MDI (–∏–∑ Excel —Ñ–∞–π–ª–∞), –∞–∫—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π (BASF, Covestro, Wanhua, Huntsman), —Ü–µ–Ω—ã –Ω–∞ —Å—ã—Ä—å–µ (–∞–Ω–∏–ª–∏–Ω, –≥–∞–∑) –∏ —Å–∫–æ—Ä —Å–æ–±—ã—Ç–∏–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π.
        2. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:** –í—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã (`look_back`).
        3. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:**
           - –î–≤–∞ —Å–ª–æ—è LSTM —Å `Dropout` –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.
           - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞.
        4. **–û–±—É—á–µ–Ω–∏–µ:** –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –æ—à–∏–±–∫—É (MSE) –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–∞–º–∏.
        5. **–ü—Ä–æ–≥–Ω–æ–∑:** –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.
        **–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏:**
        - **look_back:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–Ω–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è.
        - **–≠–ø–æ—Ö–∏:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É –æ–±—É—á–∞—é—â–µ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö.
        - **–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—å—é –∑–∞ –æ–¥–∏–Ω —à–∞–≥.
        - **–ù–µ–π—Ä–æ–Ω—ã:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ LSTM.
        - **Dropout:** –î–æ–ª—è –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º "–æ—Ç–∫–ª—é—á–∞—é—â–∏—Ö—Å—è" –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
        - **–î–æ–ª—è –æ–±—É—á–µ–Ω–∏—è:** –ü—Ä–æ—Ü–µ–Ω—Ç –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ—Å—Ç–∞–ª—å–Ω–æ–µ - –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è).
        """)
    with tab6:
        st.subheader("–õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã")
        st.info("–í—Å–µ –ª–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã —Å–æ–±—Ä–∞–Ω—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ")
        if st.session_state.log_messages:
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏–π—Å—è —Å–ø–∏—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–æ–≥–∞
            for i, log_message in enumerate(st.session_state.log_messages):
                with st.expander(f"–õ–æ–≥ #{i+1}: {log_message[:50]}..."):
                    st.text(log_message)
        else:
            st.info("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–æ–≥–æ–≤")
        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏"):
            st.session_state.log_messages = []
            st.experimental_rerun()
# --- –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
if __name__ == "__main__":
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ HARDCODED_NEWSAPI_KEY –¥–æ—Å—Ç—É–ø–µ–Ω
    try:
        from news_rss_api_upd5 import HARDCODED_NEWSAPI_KEY
    except ImportError:
        HARDCODED_NEWSAPI_KEY = 'f7e7ee9e68c64bebab1f6447c9ecd67b'      
    main()